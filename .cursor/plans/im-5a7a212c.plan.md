<!-- 5a7a212c-7b22-4a76-ae4c-7e1639b01722 eda3bcee-23b3-4d84-8a17-7094672e00bd -->
# Kế hoạch cải thiện Retrieval Quality

1) Rà soát hiện trạng đo lường

- Đọc logic tính metric hiện tại trong `monitoring/dashboard.py`, `monitoring/README.md`, `MONITORING_IMPLEMENTATION.md`, và pipeline ở `rag/pipeline.py` để hiểu cách lấy dữ liệu và log hit-rate.

2) Thêm benchmark và đánh giá ngoại sinh

- Tích hợp bộ đánh giá từ repo mở (gợi ý: BEIR hoặc MIRACL via `pybeir`/`beir` hoặc HuggingFace Datasets) để chạy batch offline trên cùng retriever, ghi lại recall@k/MRR/nDCG.
- Viết script/batch job (ví dụ `scripts/eval_retrieval.py`) để lấy mẫu dữ liệu thật + benchmark công khai, chạy, và lưu kết quả vào store (sqlite `data/metrics.db` hoặc file mới).

3) Nâng cấp logging & storage

- Mở rộng schema/ghi nhận trong `data/metrics.db` (hoặc bảng mới) để lưu per-run/per-dataset metrics (recall@k, MRR, nDCG, latency phân vị).
- Cập nhật instrumentation ở `rag/pipeline.py` để log query-level outcome (hit/miss, top-k scores) vào monitoring sink.

4) Cập nhật dashboard

- Hiển thị các metric mới (recall@k, MRR/nDCG) và so sánh theo dataset/run trong `monitoring/dashboard.py` cùng tài liệu hướng dẫn ở `monitoring/README.md`.

5) Tài liệu & hướng dẫn chạy

- Viết hướng dẫn chạy benchmark và giải thích metric mới trong `MONITORING_IMPLEMENTATION.md` hoặc `monitoring/README.md`.

### To-dos

- [ ] Rà soát code/tài liệu monitoring & pipeline
- [ ] Thêm job eval retriever với bộ benchmark mở
- [ ] Lưu metric mới vào storage/db
- [ ] Hiển thị metric mới và cập nhật hướng dẫn